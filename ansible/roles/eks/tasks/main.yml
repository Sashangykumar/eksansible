---
- name: Create VPC
  amazon.aws.ec2_vpc_net:
    name: "{{ cluster_name }}-vpc"
    cidr_block: "{{ vpc_cidr }}"
    region: "{{ region }}"
    tags:
      Environment: "{{ cluster_name }}"
    state: present
  register: vpc

- name: Create Internet Gateway
  amazon.aws.ec2_vpc_igw:
    vpc_id: "{{ vpc.vpc.id }}"
    region: "{{ region }}"
    tags:
      Name: "{{ cluster_name }}-igw"
    state: present
  register: igw

- name: Create public subnets
  amazon.aws.ec2_vpc_subnet:
    vpc_id: "{{ vpc.vpc.id }}"
    cidr: "{{ item.cidr }}"
    region: "{{ region }}"
    az: "{{ region }}{{ item.az }}"
    map_public: true
    tags:
      Name: "{{ cluster_name }}-public-{{ item.az }}"
      kubernetes.io/role/elb: "1"
    state: present
  loop:
    - { cidr: "{{ public_subnet_cidrs[0] }}", az: "a" }
    - { cidr: "{{ public_subnet_cidrs[1] }}", az: "b" }
  register: public_subnets

- name: Create private subnets
  amazon.aws.ec2_vpc_subnet:
    vpc_id: "{{ vpc.vpc.id }}"
    cidr: "{{ item.cidr }}"
    region: "{{ region }}"
    az: "{{ region }}{{ item.az }}"
    tags:
      Name: "{{ cluster_name }}-private-{{ item.az }}"
      kubernetes.io/role/internal-elb: "1"
    state: present
  loop:
    - { cidr: "{{ private_subnet_cidrs[0] }}", az: "a" }
    - { cidr: "{{ private_subnet_cidrs[1] }}", az: "b" }
  register: private_subnets

- name: Create route table for public subnets
  amazon.aws.ec2_vpc_route_table:
    vpc_id: "{{ vpc.vpc.id }}"
    region: "{{ region }}"
    tags:
      Name: "{{ cluster_name }}-public-rt"
    subnets:
      - "{{ public_subnets.results[0].subnet.id }}"
      - "{{ public_subnets.results[1].subnet.id }}"
    routes:
      - dest: "0.0.0.0/0"
        gateway_id: "{{ igw.gateway_id }}"

- name: Create EKS cluster service role
  amazon.aws.iam_role:
    name: "{{ cluster_name }}-cluster-role"
    assume_role_policy_document: |
      {
        "Version": "2012-10-17",
        "Statement": [
          {
            "Effect": "Allow",
            "Principal": {
              "Service": "eks.amazonaws.com"
            },
            "Action": "sts:AssumeRole"
          }
        ]
      }
    managed_policies:
      - arn:aws:iam::aws:policy/AmazonEKSClusterPolicy
    state: present

- name: Create EKS node group service role
  amazon.aws.iam_role:
    name: "{{ cluster_name }}-node-role"
    assume_role_policy_document: |
      {
        "Version": "2012-10-17",
        "Statement": [
          {
            "Effect": "Allow",
            "Principal": {
              "Service": "ec2.amazonaws.com"
            },
            "Action": "sts:AssumeRole"
          }
        ]
      }
    managed_policies:
      - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
      - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
      - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
    state: present

- name: Debug cluster creation command
  debug:
    msg: |
      Creating cluster with:
      Name: {{ cluster_name }}
      Role: arn:aws:iam::{{ ansible_account_id }}:role/{{ cluster_name }}-cluster-role
      Subnets: {{ public_subnets.results[0].subnet.id }},{{ public_subnets.results[1].subnet.id }},{{ private_subnets.results[0].subnet.id }},{{ private_subnets.results[1].subnet.id }}

- name: Create EKS cluster
  shell: |
    set -x
    aws eks create-cluster \
      --name {{ cluster_name }} \
      --version {{ kubernetes_version }} \
      --role-arn arn:aws:iam::{{ ansible_account_id }}:role/{{ cluster_name }}-cluster-role \
      --resources-vpc-config subnetIds={{ public_subnets.results[0].subnet.id }},{{ public_subnets.results[1].subnet.id }},{{ private_subnets.results[0].subnet.id }},{{ private_subnets.results[1].subnet.id }} \
      --region {{ region }}
  register: cluster_create_result
  ignore_errors: true

- name: Debug cluster creation result
  debug:
    var: cluster_create_result

- name: Fail if cluster creation failed with unexpected error
  fail:
    msg: "Cluster creation failed: {{ cluster_create_result.stderr }}"
  when: 
    - cluster_create_result.rc != 0
    - "'already exists' not in cluster_create_result.stderr"
    - "'AlreadyExistsException' not in cluster_create_result.stderr"

- name: Wait for EKS cluster to be active
  shell: |
    for i in {1..40}; do
      if aws eks describe-cluster --name {{ cluster_name }} --region {{ region }} >/dev/null 2>&1; then
        STATUS=$(aws eks describe-cluster --name {{ cluster_name }} --region {{ region }} --query 'cluster.status' --output text)
        echo "Cluster status: $STATUS"
        if [ "$STATUS" = "ACTIVE" ]; then
          echo "Cluster is active"
          exit 0
        fi
      else
        echo "Cluster not found yet, waiting..."
      fi
      sleep 30
    done
    echo "Timeout waiting for cluster"
    exit 1

- name: Create EKS node group
  shell: |
    aws eks create-nodegroup \
      --cluster-name {{ cluster_name }} \
      --nodegroup-name {{ node_group_name }} \
      --node-role arn:aws:iam::{{ ansible_account_id }}:role/{{ cluster_name }}-node-role \
      --subnets {{ private_subnets.results[0].subnet.id }} {{ private_subnets.results[1].subnet.id }} \
      --instance-types {{ node_instance_type }} \
      --scaling-config minSize={{ node_min_size }},maxSize={{ node_max_size }},desiredSize={{ node_desired_capacity }} \
      --region {{ region }}
  register: nodegroup_create_result
  ignore_errors: true

- name: Fail if nodegroup creation failed with unexpected error
  fail:
    msg: "Nodegroup creation failed: {{ nodegroup_create_result.stderr }}"
  when: 
    - nodegroup_create_result.rc != 0
    - "'already exists' not in nodegroup_create_result.stderr"
    - "'AlreadyExistsException' not in nodegroup_create_result.stderr"

- name: Wait for node group to be active
  shell: |
    for i in {1..40}; do
      if aws eks describe-nodegroup --cluster-name {{ cluster_name }} --nodegroup-name {{ node_group_name }} --region {{ region }} >/dev/null 2>&1; then
        STATUS=$(aws eks describe-nodegroup --cluster-name {{ cluster_name }} --nodegroup-name {{ node_group_name }} --region {{ region }} --query 'nodegroup.status' --output text)
        echo "Nodegroup status: $STATUS"
        if [ "$STATUS" = "ACTIVE" ]; then
          echo "Nodegroup is active"
          exit 0
        fi
      else
        echo "Nodegroup not found yet, waiting..."
      fi
      sleep 30
    done
    echo "Timeout waiting for nodegroup"
    exit 1

- name: Generate kubeconfig
  shell: |
    aws eks update-kubeconfig --region {{ region }} --name {{ cluster_name }}
  environment:
    AWS_DEFAULT_REGION: "{{ region }}"

- name: Get cluster information
  shell: aws eks describe-cluster --name {{ cluster_name }} --region {{ region }}
  register: cluster_info_raw

- name: Display cluster information
  debug:
    msg:
      - "EKS Cluster Name: {{ cluster_name }}"
      - "Region: {{ region }}"
      - "Cluster ARN: {{ (cluster_info_raw.stdout | from_json).cluster.arn }}"
      - "Endpoint: {{ (cluster_info_raw.stdout | from_json).cluster.endpoint }}"
      - "Kubeconfig updated at ~/.kube/config"