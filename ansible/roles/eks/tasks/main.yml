---
- name: Create VPC
  amazon.aws.ec2_vpc_net:
    name: "{{ cluster_name }}-vpc"
    cidr_block: "{{ vpc_cidr }}"
    region: "{{ region }}"
    tags:
      Environment: "{{ cluster_name }}"
    state: present
  register: vpc

- name: Create Internet Gateway
  amazon.aws.ec2_vpc_igw:
    vpc_id: "{{ vpc.vpc.id }}"
    region: "{{ region }}"
    tags:
      Name: "{{ cluster_name }}-igw"
    state: present
  register: igw

- name: Create public subnets
  amazon.aws.ec2_vpc_subnet:
    vpc_id: "{{ vpc.vpc.id }}"
    cidr: "{{ item.cidr }}"
    region: "{{ region }}"
    az: "{{ region }}{{ item.az }}"
    map_public: true
    tags:
      Name: "{{ cluster_name }}-public-{{ item.az }}"
      kubernetes.io/role/elb: "1"
    state: present
  loop:
    - { cidr: "{{ public_subnet_cidrs[0] }}", az: "a" }
    - { cidr: "{{ public_subnet_cidrs[1] }}", az: "b" }
  register: public_subnets

- name: Create private subnets
  amazon.aws.ec2_vpc_subnet:
    vpc_id: "{{ vpc.vpc.id }}"
    cidr: "{{ item.cidr }}"
    region: "{{ region }}"
    az: "{{ region }}{{ item.az }}"
    tags:
      Name: "{{ cluster_name }}-private-{{ item.az }}"
      kubernetes.io/role/internal-elb: "1"
    state: present
  loop:
    - { cidr: "{{ private_subnet_cidrs[0] }}", az: "a" }
    - { cidr: "{{ private_subnet_cidrs[1] }}", az: "b" }
  register: private_subnets

- name: Create route table for public subnets
  amazon.aws.ec2_vpc_route_table:
    vpc_id: "{{ vpc.vpc.id }}"
    region: "{{ region }}"
    tags:
      Name: "{{ cluster_name }}-public-rt"
    subnets:
      - "{{ public_subnets.results[0].subnet.id }}"
      - "{{ public_subnets.results[1].subnet.id }}"
    routes:
      - dest: "0.0.0.0/0"
        gateway_id: "{{ igw.gateway_id }}"

- name: Create EKS cluster service role
  amazon.aws.iam_role:
    name: "{{ cluster_name }}-cluster-role"
    assume_role_policy_document: |
      {
        "Version": "2012-10-17",
        "Statement": [
          {
            "Effect": "Allow",
            "Principal": {
              "Service": "eks.amazonaws.com"
            },
            "Action": "sts:AssumeRole"
          }
        ]
      }
    managed_policies:
      - arn:aws:iam::aws:policy/AmazonEKSClusterPolicy
    state: present

- name: Create EKS node group service role
  amazon.aws.iam_role:
    name: "{{ cluster_name }}-node-role"
    assume_role_policy_document: |
      {
        "Version": "2012-10-17",
        "Statement": [
          {
            "Effect": "Allow",
            "Principal": {
              "Service": "ec2.amazonaws.com"
            },
            "Action": "sts:AssumeRole"
          }
        ]
      }
    managed_policies:
      - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
      - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
      - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
    state: present

- name: Check if cluster exists
  shell: aws eks describe-cluster --name {{ cluster_name }} --region {{ region }}
  register: cluster_check
  ignore_errors: true

- name: Create EKS cluster
  shell: |
    aws eks create-cluster \
      --name {{ cluster_name }} \
      --version {{ kubernetes_version }} \
      --role-arn arn:aws:iam::{{ ansible_account_id }}:role/{{ cluster_name }}-cluster-role \
      --resources-vpc-config subnetIds={{ public_subnets.results[0].subnet.id }},{{ public_subnets.results[1].subnet.id }},{{ private_subnets.results[0].subnet.id }},{{ private_subnets.results[1].subnet.id }} \
      --region {{ region }}
  register: cluster_result
  when: cluster_check.rc != 0

- name: Wait for EKS cluster to be active
  shell: aws eks wait cluster-active --name {{ cluster_name }} --region {{ region }}
  when: cluster_check.rc != 0 and cluster_result.rc == 0

- name: Check if nodegroup exists
  shell: aws eks describe-nodegroup --cluster-name {{ cluster_name }} --nodegroup-name {{ node_group_name }} --region {{ region }}
  register: nodegroup_check
  ignore_errors: true

- name: Create EKS node group
  shell: |
    aws eks create-nodegroup \
      --cluster-name {{ cluster_name }} \
      --nodegroup-name {{ node_group_name }} \
      --node-role arn:aws:iam::{{ ansible_account_id }}:role/{{ cluster_name }}-node-role \
      --subnets {{ private_subnets.results[0].subnet.id }} {{ private_subnets.results[1].subnet.id }} \
      --instance-types {{ node_instance_type }} \
      --scaling-config minSize={{ node_min_size }},maxSize={{ node_max_size }},desiredSize={{ node_desired_capacity }} \
      --region {{ region }}
  register: nodegroup_result
  when: nodegroup_check.rc != 0

- name: Wait for node group to be active
  shell: aws eks wait nodegroup-active --cluster-name {{ cluster_name }} --nodegroup-name {{ node_group_name }} --region {{ region }}
  when: nodegroup_check.rc != 0 and nodegroup_result.rc == 0

- name: Generate kubeconfig
  shell: |
    aws eks update-kubeconfig --region {{ region }} --name {{ cluster_name }}
  environment:
    AWS_DEFAULT_REGION: "{{ region }}"

- name: Get cluster endpoint
  shell: aws eks describe-cluster --name {{ cluster_name }} --region {{ region }} --query 'cluster.endpoint' --output text
  register: cluster_endpoint

- name: Display cluster information
  debug:
    msg:
      - "EKS Cluster Name: {{ cluster_name }}"
      - "Region: {{ region }}"
      - "Endpoint: {{ cluster_endpoint.stdout }}"
      - "Kubeconfig updated at ~/.kube/config"